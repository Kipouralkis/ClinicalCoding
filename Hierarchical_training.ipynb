{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ihear\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import BertModel, BertTokenizer, AdamW, BertConfig\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "from HierarchyClassifer import HierarchyClassifier, HierarchyDataset, MentionPooling, ContextTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "\n",
    "# load annotations\n",
    "data = pd.read_csv(\"Data\\split_data.csv\")\n",
    "\n",
    "# load code-block mapping\n",
    "mapping = pd.read_csv(\"Data\\code_block_unique_mapping.csv\")\n",
    "\n",
    "# load the full texts to extract the context\n",
    "text_data = pd.read_csv(\"Data\\Final_texts.csv\")\n",
    "\n",
    "# incorporate the code-block mapping into the dataset\n",
    "data = pd.merge(data, mapping, how='left', left_on='code', right_on='code')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add context columns\n",
    "\n",
    "def extract_context(text, start_position, end_position, context_size=5):\n",
    "      # Ensure valid positions\n",
    "        if start_position < 0 or end_position > len(text) or start_position >= end_position:\n",
    "            raise ValueError(\"Invalid mention positions\")\n",
    "\n",
    "        # extract mention\n",
    "        mention = text[start_position:end_position]\n",
    "\n",
    "        # get the left context and tokenize\n",
    "        left_context = text[:start_position]\n",
    "        left_context = left_context.split()[-context_size:]\n",
    "\n",
    "        # get the right context and tokenize\n",
    "        right_context = text[end_position:]\n",
    "        right_context = right_context.split()[:context_size]\n",
    "\n",
    "        return mention, \" \".join(left_context), \" \".join(right_context)\n",
    "\n",
    "\n",
    "# itarate data and extract the context for each mention\n",
    "for _, row_texts in text_data.iterrows():\n",
    "    for _, row_annotations in data.iterrows():\n",
    "        if row_texts.patient_id == row_annotations.patient_id:\n",
    "            mention, left_context, right_context = extract_context(row_texts.text, row_annotations.start, row_annotations.end)\n",
    "            # Add columns to the data DataFrame\n",
    "            data.at[row_annotations.name, 'left_context'] = left_context\n",
    "            data.at[row_annotations.name, 'right_context'] = right_context\n",
    "        else:\n",
    "             continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set splits \n",
    "\n",
    "train_df = data[data['set'] == 'train']\n",
    "test_df = data[data['set'] == 'test']\n",
    "val_df = data[data['set'] == 'validation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize data\n",
    "\n",
    "max_length = 128\n",
    "\n",
    "# initialize custom tokenizer from Bert pre-trained\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-uncased')\n",
    "ct = ContextTokenizer(tokenizer, special_tokens={'additional_special_tokens': ['[Ms]','[Me]']}, max_length=max_length)\n",
    "\n",
    "def tokenize_data(df, tokenizer_instance):\n",
    "    tokenized_data = df.apply(tokenizer_instance.tokenizeWcontext, axis=1)\n",
    "    return list(tokenized_data)\n",
    "\n",
    "tokenized_train = tokenize_data(train_df, ct)\n",
    "tokenized_test = tokenize_data(test_df, ct)\n",
    "tokenized_val = tokenize_data(val_df, ct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# special tokens mask created to locate the positions of the special tokens to used for the pooling function\n",
    "\n",
    "def create_special_tokens_mask(input_ids, tokenizer, special_tokens=[\"[Ms]\", \"[Me]\"]):\n",
    "    # convert input_ids to a PyTorch tensor\n",
    "    input_ids = torch.Tensor(input_ids)\n",
    "\n",
    "    # get the token IDs for the special tokens\n",
    "    special_token_ids = tokenizer.convert_tokens_to_ids(special_tokens)\n",
    "\n",
    "    # create a mask indicating the positions of special tokens\n",
    "    special_tokens_mask = torch.zeros_like(input_ids, dtype=torch.bool)\n",
    "\n",
    "    for token_id in special_token_ids:\n",
    "        special_tokens_mask |= (input_ids == token_id)\n",
    "\n",
    "    return special_tokens_mask\n",
    "\n",
    "\n",
    "# Create a list to store special tokens masks\n",
    "train_special_tokens_masks = []\n",
    "test_special_tokens_masks = []\n",
    "val_special_tokens_masks = []\n",
    "\n",
    "for tokenized_input in tokenized_train:\n",
    "    input_ids = tokenized_input['input_ids']\n",
    "    special_tokens_mask = create_special_tokens_mask(input_ids, ct.tokenizer)\n",
    "    train_special_tokens_masks.append(special_tokens_mask)\n",
    "\n",
    "for tokenized_input in tokenized_test:\n",
    "    input_ids = tokenized_input['input_ids']\n",
    "    special_tokens_mask = create_special_tokens_mask(input_ids, ct.tokenizer)\n",
    "    test_special_tokens_masks.append(special_tokens_mask)\n",
    "\n",
    "for tokenized_input in tokenized_val:\n",
    "    input_ids = tokenized_input['input_ids']\n",
    "    special_tokens_mask = create_special_tokens_mask(input_ids, ct.tokenizer)\n",
    "    val_special_tokens_masks.append(special_tokens_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map the labels to integer ranges and create labelsets\n",
    "\n",
    "parent_class_to_index = {class_name: index for index, class_name in enumerate(data['block'].unique())}\n",
    "child_class_to_index = {class_name: index for index, class_name in enumerate(data['code'].unique())}\n",
    "\n",
    "# Create lists of indices for parent and child labels\n",
    "train_parent_labels = [parent_class_to_index[label] for label in train_df['block']]\n",
    "train_child_labels = [child_class_to_index[label] for label in train_df['code']]\n",
    "\n",
    "test_parent_labels = [parent_class_to_index[label] for label in test_df['block']]\n",
    "test_child_labels = [child_class_to_index[label] for label in test_df['code']]\n",
    "\n",
    "val_parent_labels = [parent_class_to_index[label] for label in val_df['block']]\n",
    "val_child_labels = [child_class_to_index[label] for label in val_df['code']]\n",
    "\n",
    "train_labels = [[child, parent] for child, parent in zip(train_child_labels, train_parent_labels)]\n",
    "val_labels = [[child, parent] for child, parent in zip(val_child_labels, val_parent_labels)]\n",
    "test_labels = [[child, parent] for child, parent in zip(test_child_labels, test_parent_labels)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = HierarchyDataset(tokenized_train, train_labels, train_special_tokens_masks)\n",
    "test_dataset = HierarchyDataset(tokenized_test, test_labels, test_special_tokens_masks)\n",
    "val_dataset = HierarchyDataset(tokenized_val, val_labels, val_special_tokens_masks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataLoaders\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n",
    "val_loader = DataLoader(dataset=val_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    
   ],
   "source": [
    "model = BertModel.from_pretrained('bert-base-multilingual-uncased')\n",
    "\n",
    "# Resize token embeddings\n",
    "model.resize_token_embeddings(len(ct.tokenizer))\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=1e-5, weight_decay=0.0)\n",
    "config = BertConfig.from_pretrained('bert-base-multilingual-uncased')\n",
    "parent_classifier = nn.Linear(config.hidden_size, data['block'].nunique())\n",
    "child_classifier = nn.Linear(config.hidden_size, data['code'].nunique())\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# set optimizers for the linear layers\n",
    "optimizer_parent = torch.optim.Adam(parent_classifier.parameters(), lr=1e-3)\n",
    "optimizer_child = torch.optim.Adam(child_classifier.parameters(), lr=1e-3)\n",
    "\n",
    "average_pooling_model = HierarchyClassifier(model=model,\n",
    "                                 optimizer=optimizer,\n",
    "                                 parent_classifier=parent_classifier,\n",
    "                                 child_classifier=child_classifier,\n",
    "                                 optimizer_parent = optimizer_parent,\n",
    "                                 optimizer_child = optimizer_child,\n",
    "                                 criterion=criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pooling_function = MentionPooling(pool_type='average')\n",
    "\n",
    "average_pooling_model.train(train_dataloader=train_loader,\n",
    "                      val_dataloader=val_loader,\n",
    "                      num_epochs=6,\n",
    "                      foldername='prev_model',\n",
    "                      paren_weight= 0.9,\n",
    "                      train_parent=True,\n",
    "                      pooling_function=pooling_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "\n",
    "# average_pooling_model.load_model('prev_model\\checkpoint_epoch_5.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation: 100%|██████████| 25/25 [03:10<00:00,  7.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation metrics\n",
      "Eval Avg Loss: 0.025794467602303896, Child Accuracy: 0.8108108108108109, Parent Accuracy 0.9317889317889317\n",
      "Macro, Average\n",
      "Eval Child Precision: 0.5383369892361333, Eval Child Recall: 0.5448285247444841, Eval Child F1: 0.534431898223665\n",
      "Eval Parent Precision: 0.7211277173913043, Eval Parent Recall: 0.6863920673877083, Eval Parent F1: 0.6953271586474395\n",
      "\n",
      "Micro, Average\n",
      "Eval Child Precision: 0.8108108108108109, Eval Child Recall: 0.8108108108108109, Eval Child F1: 0.8108108108108109\n",
      "Eval Parent Precision: 0.9317889317889317, Eval Parent Recall: 0.9317889317889317, Eval Parent F1: 0.9317889317889317\n",
      "\n",
      "Weighted, Average\n",
      "Eval Child Precision: 0.7896622711968275, Eval Child Recall: 0.8108108108108109, Eval Child F1: 0.7948467502235301\n",
      "Eval Parent Precision: 0.9242241187505161, Eval Parent Recall: 0.9317889317889317, Eval Parent F1: 0.9260974172882394\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "c:\\Users\\ihear\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\ihear\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\ihear\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\ihear\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\ihear\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\ihear\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\ihear\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\ihear\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# evaluation\n",
    "device = 'cpu'\n",
    "\n",
    "average_pooling_model.model.eval()\n",
    "total_loss = 0.0\n",
    "total_child = 0.0\n",
    "total_child_correct = 0.0\n",
    "total_child_samples = 0.0\n",
    "all_true_child_labels = []\n",
    "all_pred_child_labels = []\n",
    "\n",
    "all_true_parent_labels = []\n",
    "all_pred_parent_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_loader, desc='Evaluation', leave=True):\n",
    "        # Extract tensors from the batch dictionary\n",
    "        input_ids = batch['input_ids'].to(device).squeeze(1)\n",
    "        attention_mask = batch['attention_mask'].to(device).squeeze(1)\n",
    "        token_type_ids = batch['token_type_ids'].to(device).squeeze(1)\n",
    "        special_token_masks = batch['special_token_mask']\n",
    "        parent_labels = batch['parent_label'].to(device)\n",
    "        child_labels = batch['child_label'].to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = average_pooling_model.model(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "\n",
    "        logits_parent = average_pooling_model.parent_classifier(outputs.pooler_output)\n",
    "        pooled_mentions = pooling_function(outputs.last_hidden_state, special_token_masks)\n",
    "        logits_child = average_pooling_model.child_classifier(pooled_mentions)\n",
    "        \n",
    "        # Calculate losses\n",
    "        loss_child = average_pooling_model.criterion(logits_child, child_labels)\n",
    "\n",
    "        loss = loss_child\n",
    "        total_loss += loss.item()\n",
    "        total_child += loss_child\n",
    "\n",
    "        _, predicted_parent = torch.max(logits_parent, 1)\n",
    "        _, predicted_child = torch.max(logits_child, 1)\n",
    "\n",
    "        total_child_correct += (predicted_child == child_labels).sum().item()\n",
    "\n",
    "        total_child_samples += child_labels.size(0)\n",
    "\n",
    "        # Store true and predicted labels for precision, recall, and F1 score\n",
    "        all_true_child_labels.extend(child_labels.cpu().numpy())\n",
    "        all_pred_child_labels.extend(predicted_child.cpu().numpy())\n",
    "\n",
    "        all_true_parent_labels.extend(parent_labels.cpu().numpy())\n",
    "        all_pred_parent_labels.extend(predicted_parent.cpu().numpy())\n",
    "\n",
    "    # Calculate average loss and accuracy\n",
    "    average_loss = total_loss / len(test_df)\n",
    "    accuracy_child = accuracy_score(all_true_child_labels, all_pred_child_labels)\n",
    "    accuracy_parent = accuracy_score(all_true_parent_labels, all_pred_parent_labels)\n",
    "\n",
    "    print()\n",
    "    print(\"Evaluation metrics\")\n",
    "    print(f'Eval Avg Loss: {average_loss}, Child Accuracy: {accuracy_child}, Parent Accuracy {accuracy_parent}')\n",
    "    print()\n",
    "\n",
    "    def calc_metrics(average):\n",
    "\n",
    "        # Calculate precision, recall, and F1 score\n",
    "        precision_child = precision_score(all_true_child_labels, all_pred_child_labels, average=average)\n",
    "        recall_child = recall_score(all_true_child_labels, all_pred_child_labels, average=average)\n",
    "        f1_child = f1_score(all_true_child_labels, all_pred_child_labels, average=average)\n",
    "        \n",
    "        precision_parent = precision_score(all_true_parent_labels, all_pred_parent_labels, average=average)\n",
    "        recall_parent = recall_score(all_true_parent_labels, all_pred_parent_labels, average=average)\n",
    "        f1_parent = f1_score(all_true_parent_labels, all_pred_parent_labels, average=average)\n",
    "\n",
    "        print(f'{average.capitalize()} Average')\n",
    "        print(f'Eval Child Precision: {precision_child}, Eval Child Recall: {recall_child}, Eval Child F1: {f1_child}')\n",
    "        print(f'Eval Parent Precision: {precision_parent}, Eval Parent Recall: {recall_parent}, Eval Parent F1: {f1_parent}')\n",
    "        print()\n",
    "\n",
    "    calc_metrics('macro')\n",
    "    calc_metrics('micro')\n",
    "    calc_metrics('weighted')\n",
    "   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
