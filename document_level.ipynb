{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make predictions based on document level labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import BertModel, BertTokenizer, AdamW, BertConfig\n",
    "\n",
    "from HierarchyClassifer import HierarchyClassifier, HierarchyDataset, MentionPooling, ContextTokenizer\n",
    "from ICD10Encoder import ICD10BiEncoder, EntityMentionTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prerare the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Unnamed: 0', 'patient_id', 'code', 'start', 'end', 'text', 'base_code',\n",
      "       'chapter', 'description', 'set'],\n",
      "      dtype='object')\n",
      "(3886, 10)\n",
      "Index(['code', 'block', 'index', 'block_index'], dtype='object')\n",
      "Index(['Unnamed: 0', 'patient_id', 'code', 'start', 'end', 'text', 'base_code',\n",
      "       'chapter', 'description', 'set', 'block', 'index', 'block_index'],\n",
      "      dtype='object')\n",
      "(3886, 13)\n"
     ]
    }
   ],
   "source": [
    "# load data \n",
    "\n",
    "data = pd.read_csv(\"Data\\split_data.csv\")\n",
    "mapping = pd.read_csv(\"Data\\code_block_unique_mapping.csv\")\n",
    "text_data = pd.read_csv(\"Data\\Final_texts.csv\")\n",
    "\n",
    "data = pd.merge(data, mapping, how='left', left_on='code', right_on='code')\n",
    "\n",
    "# split test set\n",
    "test_df = data[data['set'] == 'test']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extarct labels of each document\n",
    "\n",
    "def extract_context(text, start_position, end_position, context_size=5):\n",
    "      # Ensure valid positions\n",
    "        if start_position < 0 or end_position > len(text) or start_position >= end_position:\n",
    "            raise ValueError(\"Invalid mention positions\")\n",
    "\n",
    "        # Extract mention\n",
    "        mention = text[start_position:end_position]\n",
    "\n",
    "        # get the left context and tokenize\n",
    "        left_context = text[:start_position]\n",
    "        left_context = left_context.split()[-context_size:]\n",
    "\n",
    "        # get the right context and tokenize\n",
    "        right_context = text[end_position:]\n",
    "        right_context = right_context.split()[:context_size]\n",
    "\n",
    "        return mention, \" \".join(left_context), \" \".join(right_context)\n",
    "\n",
    "doc_labels = []\n",
    "\n",
    "for _, row_texts in text_data.iterrows():\n",
    "    for _, row_annotations in data.iterrows():\n",
    "        if row_texts.patient_id == row_annotations.patient_id:\n",
    "            mention, left_context, right_context = extract_context(row_texts.text, row_annotations.start, row_annotations.end)\n",
    "            # Add columns to the data DataFrame\n",
    "            data.at[row_annotations.name, 'left_context'] = left_context\n",
    "            data.at[row_annotations.name, 'right_context'] = right_context\n",
    "        else:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare dataset by adding the document labels\n",
    "\n",
    "grouped_df = data.groupby('patient_id').agg(list).reset_index()\n",
    "merged_df = pd.merge(data, grouped_df[['patient_id', 'code']], on='patient_id', how='left', suffixes=('', '_grouped'))\n",
    "\n",
    "test_df = merged_df[merged_df['set'] == 'test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize tokenizer \n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-uncased')\n",
    "ct = ContextTokenizer(tokenizer, special_tokens={'additional_special_tokens': ['[Ms]','[Me]']}, max_length=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_data(df, tokenizer_instance):\n",
    "    tokenized_data = df.apply(tokenizer_instance.tokenizeWcontext, axis=1)\n",
    "    return list(tokenized_data)\n",
    "\n",
    "tokenized_test = tokenize_data(test_df, ct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_special_tokens_mask(input_ids, tokenizer, special_tokens=[\"[Ms]\", \"[Me]\"]):\n",
    "    # Convert input_ids to a PyTorch tensor\n",
    "    input_ids = torch.Tensor(input_ids)\n",
    "\n",
    "    # Get the token IDs for the special tokens\n",
    "    special_token_ids = tokenizer.convert_tokens_to_ids(special_tokens)\n",
    "\n",
    "    # Create a mask indicating the positions of special tokens\n",
    "    special_tokens_mask = torch.zeros_like(input_ids, dtype=torch.bool)\n",
    "\n",
    "    for token_id in special_token_ids:\n",
    "        special_tokens_mask |= (input_ids == token_id)\n",
    "\n",
    "    return special_tokens_mask\n",
    "\n",
    "\n",
    "# Create a list to store special tokens masks\n",
    "test_special_tokens_masks = []\n",
    "\n",
    "for tokenized_input in tokenized_test:\n",
    "    input_ids = tokenized_input['input_ids']\n",
    "    special_tokens_mask = create_special_tokens_mask(input_ids, ct.tokenizer)\n",
    "    test_special_tokens_masks.append(special_tokens_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "parent_class_to_index = {class_name: index for index, class_name in enumerate(data['block'].unique())}\n",
    "child_class_to_index = {class_name: index for index, class_name in enumerate(data['code'].unique())}\n",
    "\n",
    "test_parent_labels = [parent_class_to_index[label] for label in test_df['block']]\n",
    "test_child_labels = [child_class_to_index[label] for label in test_df['code']]\n",
    "\n",
    "test_labels = [[child, parent] for child, parent in zip(test_child_labels, test_parent_labels)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
   
   ],
   "source": [
    "code_to_description_mapping = dict(zip(data['code'], data['description']))\n",
    "print(code_to_description_mapping)\n",
    "print(child_class_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exctract code groups\n",
    "\n",
    "codes = test_df['code_grouped'].tolist()\n",
    "codes = [set(item) for item in codes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataloader\n",
    "\n",
    "test_dataset = HierarchyDataset(tokenized_test, test_labels, test_special_tokens_masks)\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifier Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    
   ],
   "source": [
    "# initialize classifier model\n",
    "\n",
    "model = BertModel.from_pretrained('bert-base-multilingual-uncased')\n",
    "\n",
    "# Resize token embeddings\n",
    "model.resize_token_embeddings(len(ct.tokenizer))\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=1e-5)\n",
    "config = BertConfig.from_pretrained('bert-base-multilingual-uncased')\n",
    "parent_classifier = nn.Linear(config.hidden_size, data['block'].nunique())\n",
    "child_classifier = nn.Linear(config.hidden_size, data['code'].nunique())\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# set optimizers for the linear layers\n",
    "optimizer_parent = torch.optim.Adam(parent_classifier.parameters(), lr=1e-3)\n",
    "optimizer_child = torch.optim.Adam(child_classifier.parameters(), lr=1e-3)\n",
    "\n",
    "pooling_function = MentionPooling(pool_type='average')\n",
    "\n",
    "average_pooling_model = HierarchyClassifier(model=model,\n",
    "                                 optimizer=optimizer,\n",
    "                                 parent_classifier=parent_classifier,\n",
    "                                 child_classifier=child_classifier,\n",
    "                                 optimizer_parent = optimizer_parent,\n",
    "                                 optimizer_child = optimizer_child,\n",
    "                                 criterion=criterion)\n",
    "\n",
    "average_pooling_model.load_model('Hier_CONTEXT_6th\\checkpoint_epoch_0.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate test set\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "with torch.no_grad():\n",
    "    all_true_child_labels = []\n",
    "    all_pred_child_labels = []\n",
    "    i = 0\n",
    "\n",
    "    for mention in tqdm(tokenized_test, desc='Evaluation', leave=True):\n",
    "\n",
    "        # retrieve candidate concepts\n",
    "        cc = codes[i]\n",
    "        candidate_indexes = [child_class_to_index[code] for code in cc]\n",
    "    \n",
    "        input_ids = mention['input_ids']\n",
    "        attention_mask = mention['attention_mask']\n",
    "        token_type_ids = mention['token_type_ids']\n",
    "        child_labels = test_labels[i][0]\n",
    "        special_tokens_mask = create_special_tokens_mask(input_ids, ct.tokenizer)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = average_pooling_model.model(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "\n",
    "        pooled_mentions = pooling_function(outputs.last_hidden_state, special_tokens_mask)\n",
    "        logits_child = average_pooling_model.child_classifier(pooled_mentions)\n",
    "        \n",
    "        # Apply softmax to get probabilities\n",
    "        probabilities = torch.nn.functional.softmax(logits_child, dim=-1)\n",
    "      \n",
    "        selected_probs = []\n",
    "        for x in candidate_indexes:\n",
    "            selected_probs.append(probabilities[0][x].item())\n",
    "\n",
    "        zipped_data = list(zip(candidate_indexes, selected_probs))\n",
    "    \n",
    "        # Sort based on probabilities\n",
    "        sorted_data = sorted(zipped_data, key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "        # Extract the sorted candidate indexes\n",
    "        sorted_candidate_indexes = [item[0] for item in sorted_data]\n",
    "       \n",
    "        predicted_label = sorted_candidate_indexes[0]\n",
    "\n",
    "        # Sort candidate concepts based on predicted probabilities\n",
    "        \n",
    "        # Update all_true_child_labels and all_pred_child_labels\n",
    "        all_true_child_labels.append(child_labels)\n",
    "\n",
    "        all_pred_child_labels.append(predicted_label)\n",
    "\n",
    "        i += 1\n",
    "\n",
    "    # Calculate precision, recall, and F1 score\n",
    "    accuracy_child = accuracy_score(all_true_child_labels, all_pred_child_labels)\n",
    "    precision_child = precision_score(all_true_child_labels, all_pred_child_labels, average='macro')\n",
    "    recall_child = recall_score(all_true_child_labels, all_pred_child_labels, average='macro')\n",
    "    f1_child = f1_score(all_true_child_labels, all_pred_child_labels, average='macro')\n",
    "\n",
    "    # Print or store evaluation metrics\n",
    "    print()\n",
    "    print(\"Evaluation Metrics\")\n",
    "    print(f'Eval Accuracy: {accuracy_child}')\n",
    "    print(f'Eval Child Precision: {precision_child}, Eval Child Recall: {recall_child}, Eval Child F1: {f1_child}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bi-encoder Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load embeddings dict\n",
    "import pickle\n",
    "\n",
    "file_path = 'Data\\embeddings_dict.pkl'\n",
    "with open(file_path, 'rb') as file:\n",
    "    entity_embeddings_dict = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-uncased')\n",
    "emt = EntityMentionTokenizer(tokenizer, max_length=128)\n",
    "\n",
    "model_mention = BertModel.from_pretrained('bert-base-multilingual-uncased')\n",
    "model_entity = BertModel.from_pretrained('bert-base-multilingual-uncased')\n",
    "\n",
    "# Resize token embeddings\n",
    "model_mention.resize_token_embeddings(len(emt.tokenizer))\n",
    "model_entity.resize_token_embeddings(len(emt.tokenizer))\n",
    "\n",
    "# Define separate optimizers for mention and entity transformers\n",
    "optimizer_mention = AdamW(model_mention.parameters(), lr=1e-5)\n",
    "optimizer_entity = AdamW(model_entity.parameters(), lr=1e-5)\n",
    "\n",
    "bi_encoder = ICD10BiEncoder(mention_transformer=model_mention,\n",
    "                            entity_trasformer=model_entity,\n",
    "                            optimizer_mention=optimizer_mention,\n",
    "                            optimizer_entity=optimizer_entity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the checkpoint\n",
    "def load_model(model, checkpoint_path):\n",
    "    # checkpoint_path = \"path/to/your/checkpoint.pt\"  # Replace with the actual path to your checkpoint\n",
    "    checkpoint = torch.load(checkpoint_path)\n",
    "\n",
    "    # Load the model state\n",
    "    model.entity_transformer.load_state_dict(checkpoint['entity_model_state_dict'])\n",
    "    model.mention_transformer.load_state_dict(checkpoint['mention_model_state_dict'])\n",
    "\n",
    "    # If you have optimizers, load their state_dicts as well\n",
    "    model.entity_optimizer.load_state_dict(checkpoint['entity_optimizer_state_dict'])\n",
    "    model.mention_optimizer.load_state_dict(checkpoint['mention_optimizer_state_dict'])\n",
    "\n",
    "    # Other information\n",
    "    current_epoch = checkpoint['epoch']\n",
    "    model.train_losses = checkpoint['train_losses']\n",
    "    model.val_losses = checkpoint['val_losses']\n",
    "    \n",
    "\n",
    "load_model(bi_encoder, 'ICD10BiEncoder.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chnaged it to filter candidate concepts based on the document labels\n",
    "\n",
    "def generate_candidates(tokenized_mention, bi_encoder, entity_embeddings, pooling_function, document_labels = 0, top_k=50, similarity_metric='dot_product'):\n",
    "   \n",
    "    input_ids = tokenized_mention['input_ids']\n",
    "    attention_mask = tokenized_mention['attention_mask']\n",
    "    token_type_ids = tokenized_mention['token_type_ids']\n",
    "    special_tokens_mask = create_special_tokens_mask(input_ids, ct.tokenizer)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        mention_outputs = bi_encoder.mention_transformer(input_ids, attention_mask, token_type_ids)\n",
    "        \n",
    "    mention_embeddings = pooling_function(mention_outputs.last_hidden_state, special_tokens_mask)\n",
    "\n",
    "    entity_tensors = tuple(entity_embeddings.values())\n",
    "  \n",
    "    similarity_scores = torch.matmul(mention_embeddings, torch.squeeze(torch.stack(entity_tensors), dim=1).t())\n",
    "    \n",
    "    selected_probabilities = similarity_scores[0, document_labels]\n",
    "\n",
    "    zipped_data = list(zip(document_labels, selected_probabilities))\n",
    "\n",
    "    sorted_data = sorted(zipped_data, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    highest_k = sorted_data[:top_k]\n",
    "\n",
    "    return highest_k\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def generate_candidates(tokenized_mention, bi_encoder, entity_embeddings, pooling_function, top_k=50, similarity_metric='dot_product'):\n",
    "   \n",
    "#     input_ids = tokenized_mention['input_ids']\n",
    "#     attention_mask = tokenized_mention['attention_mask']\n",
    "#     token_type_ids = tokenized_mention['token_type_ids']\n",
    "#     special_tokens_mask = create_special_tokens_mask(input_ids, ct.tokenizer)\n",
    "\n",
    "#     # print(input_ids)\n",
    "    \n",
    "#     # Step 2: Forward Pass\n",
    "#     with torch.no_grad():\n",
    "#         mention_outputs = bi_encoder.mention_transformer(input_ids, attention_mask, token_type_ids)\n",
    "\n",
    "\n",
    "#     mention_embeddings = pooling_function(mention_outputs.last_hidden_state, special_tokens_mask)\n",
    "#     # mention_embeddings = mention_outputs.last_hidden_state[:, 0, :]  # Assuming 'CLS' pooling\n",
    "\n",
    "#     # print(mention_embeddings.shape)\n",
    "#     entity_tensors = tuple(entity_embeddings.values())\n",
    "\n",
    "#     # get similarity score\n",
    "#     if similarity_metric == 'dot_product':\n",
    "#         similarity_scores = torch.matmul(mention_embeddings, torch.squeeze(torch.stack(entity_tensors), dim=1).t())\n",
    "#         _, top_indices = torch.topk(similarity_scores, top_k, dim=1, largest=True, sorted=True)\n",
    "#         top_candidates = [list(entity_embeddings.keys())[i.item()] for i in top_indices[0]]\n",
    "#     elif similarity_metric =='euclidean':\n",
    "#         similarity_scores = -torch.norm(torch.stack(entity_tensors) - mention_embeddings.unsqueeze(0), dim=2, p=2)\n",
    "#         _, top_indices = torch.topk(similarity_scores, top_k, dim=0, largest=True, sorted=True)\n",
    "#         top_candidates = [list(entity_embeddings.keys())[i.item()] for i in top_indices[0]]\n",
    "#     elif similarity_metric == 'jaccard':\n",
    "#         similarity_scores = F.cosine_similarity(F.relu(mention_embeddings), F.relu(torch.stack(entity_tensors)), dim=1)\n",
    "#     elif similarity_metric == 'cosine':\n",
    "#         similarity_scores = F.cosine_similarity(torch.stack(entity_tensors).squeeze(1), mention_embeddings, dim=1)\n",
    "#         # print(similarity_scores)\n",
    "#         _, top_indices = torch.topk(similarity_scores, top_k, largest=True, sorted=True)\n",
    "#         # print(top_indices.tolist())\n",
    "#         top_candidates = [list(entity_embeddings.keys())[i] for i in top_indices]\n",
    "#     # print(top_indices)\n",
    "    \n",
    "#     return top_indices #, top_candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation:  78%|███████▊  | 604/777 [03:16<00:55,  3.14it/s]"
     ]
    }
   ],
   "source": [
    "# evaluation loop\n",
    "\n",
    "with torch.no_grad():\n",
    "    all_true_child_labels = []\n",
    "    all_pred_child_labels = []\n",
    "    label_pairs = []\n",
    "    misclassified_instances = []\n",
    "    correctly_classified_instances = []\n",
    "    k_indexes = []\n",
    "    i = 0\n",
    "\n",
    "    for mention in tqdm(tokenized_test, desc='Evaluation', leave=True):\n",
    "\n",
    "\n",
    "        document_labels = [child_class_to_index[code] for code in codes[i]]\n",
    "        child_labels = test_labels[i][0]\n",
    "\n",
    "        # get dot product distance from the bi encoder embeddings\n",
    "        predicted_k = generate_candidates(mention, bi_encoder, entity_embeddings_dict, pooling_function, similarity_metric='dot_product', top_k=100)\n",
    "\n",
    "        predicted_k = predicted_k.tolist()[0]\n",
    "        \n",
    "        if child_labels in predicted_k:\n",
    "            predicted_label = child_labels\n",
    "            index = predicted_k.index(predicted_label)\n",
    "            k_indexes.append(index)\n",
    "    \n",
    "        all_true_child_labels.append(child_labels)\n",
    "        all_pred_child_labels.append(predicted_label)\n",
    "\n",
    "        def find_key_by_value(dictionary, target_value):\n",
    "            for key, value in dictionary.items():\n",
    "                if value == target_value:\n",
    "                    return key\n",
    "            return None\n",
    "        \n",
    "        true_label = find_key_by_value(child_class_to_index, child_labels)\n",
    "        true_pred = find_key_by_value(child_class_to_index, predicted_label)\n",
    "\n",
    "\n",
    "        label_pairs.append([true_label, true_pred])\n",
    "\n",
    "        # check if predicted label is correct\n",
    "        if predicted_label != child_labels:\n",
    "            mention_text = ct.tokenizer.decode(mention['input_ids'][0], skip_special_tokens=False)\n",
    "\n",
    "            # Save misclassified instance details\n",
    "            misclassified_instances.append({\n",
    "                'mention_text': mention_text,\n",
    "                'true_label': true_label,\n",
    "                'predicted_label': true_pred\n",
    "            })\n",
    "        \n",
    "        elif predicted_label == child_labels:\n",
    "\n",
    "            mention_text = ct.tokenizer.decode(mention['input_ids'][0], skip_special_tokens=False)\n",
    "\n",
    "            # Save misclassified instance details\n",
    "            correctly_classified_instances.append({\n",
    "                'mention_text': mention_text,\n",
    "                'true_label': true_label,\n",
    "                'predicted_label': true_pred\n",
    "            })\n",
    "\n",
    "\n",
    "        i += 1\n",
    "\n",
    "    # Calculate metrics\n",
    "    accuracy_child = accuracy_score(all_true_child_labels, all_pred_child_labels)\n",
    "    precision_child = precision_score(all_true_child_labels, all_pred_child_labels, average='macro')\n",
    "    recall_child = recall_score(all_true_child_labels, all_pred_child_labels, average='macro')\n",
    "    f1_child = f1_score(all_true_child_labels, all_pred_child_labels, average='macro')\n",
    "\n",
    "    print()\n",
    "    print(\"Evaluation Metrics\")\n",
    "    print(f'Eval Accuracy: {accuracy_child}')\n",
    "    print(f'Eval Child Precision: {precision_child}, Eval Child Recall: {recall_child}, Eval Child F1: {f1_child}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Misclassification Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'mention_text': '[CLS] ενζυμων. ο υπερηχοκαρδιογραφικος ελεγχος ανεδειξε [Ms] μη διατηρημενο κλασμα εξωθησεως [Me] ( 35 - 40 % ). διενεργηθηκε στεφανιογραφικος ελεγχος ο [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]',\n",
       "  'true_label': 'I50.9',\n",
       "  'predicted_label': 'Z95'},\n",
       " {'mention_text': '[CLS] υπερταση, σακχαρωδης διαβητης, [Ms] αγχωδης συνδρομη [Me], εκτακτοσυστολικη αρρυθμια πορεια νοσου [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]',\n",
       "  'true_label': 'Z99.2',\n",
       "  'predicted_label': 'I49.3'},\n",
       " {'mention_text': '[CLS] την τοποθετηση του stent εγινε [Ms] διαχωρισμος [Me] στο περιφερικο του ακρο και [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]',\n",
       "  'true_label': 'I72',\n",
       "  'predicted_label': 'I07.1'}]"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "misclassified_instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of misclassified true labels not present in the training set: 7\n",
      "Misclassified true labels not seen in training set: ['I31.8', 'Z96.6', 'I42', 'I65', 'I35', 'I70', 'Z86.7']\n"
     ]
    }
   ],
   "source": [
    "misclassified_true_labels = [entry['true_label'] for entry in misclassified_instances]\n",
    "\n",
    "train_df = data[data['set'] == 'train']\n",
    "training_labels = train_df['code'].tolist()\n",
    "\n",
    "seen_misclassified = [label for label in misclassified_true_labels if label not in training_labels]\n",
    "\n",
    "num_seen_misclassified = len(seen_misclassified)\n",
    "\n",
    "print(\"Number of misclassified true labels not present in the training set: \", num_seen_misclassified)\n",
    "print(\"Misclassified true labels not seen in training set: \", seen_misclassified)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n",
      "['I35.8', 'I31.8', 'Z96.6', 'I42', 'I65', 'D86.8', 'M25.5', 'N17', 'I51.4', 'I70.1', 'I35', 'D68.5', 'I44.6', 'I70', 'Z86.7']\n"
     ]
    }
   ],
   "source": [
    "# see if the model correctly classified any unseen classes\n",
    "\n",
    "test_labels_list = test_df['code'].to_list()\n",
    "correctly_classified_true_labels = [entry['true_label'] for entry in correctly_classified_instances]\n",
    "\n",
    "unseen_labels = set(test_labels_list) ^ set(training_labels)\n",
    "\n",
    "correct_unseen = [label for label in correctly_classified_true_labels if label in unseen_labels]\n",
    "\n",
    "print(\"Number of correctly classified true labels not present in the training set: \", len(correct_unseen))\n",
    "print(\"Correctly classifie labels not seen in training set: \", correct_unseen)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
